{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c9e66e",
   "metadata": {},
   "source": [
    "# Homework Guidelines: \n",
    "\n",
    "## Building a Custom Neural Network for MNIST Digit Classification\n",
    "\n",
    "In this homework assignment, you are tasked with understanding and possibly modifying a Python script that trains a neural network to classify handwritten digits from the MNIST dataset. The provided code uses TensorFlow and Keras. Your goal is to comprehend the code and potentially make some modifications as indicated.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1 - Importing Libraries: Understand the initial part of the code where necessary libraries are imported. Ensure that you have TensorFlow and Keras installed in your environment.\n",
    "\n",
    "2- Loading the MNIST Dataset: Observe how the MNIST dataset is loaded and divided into training and testing sets. The dataset consists of images and labels.\n",
    "\n",
    "3- Data Preprocessing: Understand the normalization of pixel values to the range [0, 1]. This preprocessing step is essential for efficient model training.\n",
    "\n",
    "4- One-Hot Encoding: Comprehend the one-hot encoding process applied to the labels. It converts class labels into a binary matrix representation. Ensure that you understand the purpose of this transformation.\n",
    "\n",
    "5- Custom Dense Layer: Examine the custom dense layer definition. This layer is used within the neural network architecture and allows you to specify the number of units and activation function.\n",
    "\n",
    "    Pay attention to how weights and biases are initialized in the build method.\n",
    "    Understand how the layer performs matrix multiplication and applies the activation function in the call method.\n",
    "6- Neural Network Architecture: Analyze the definition of the neural network model. It consists of a series of layers, including custom dense layers with specified units and activation functions.\n",
    "\n",
    "    Identify the input shape for the first layer (28x28 for MNIST images).\n",
    "    Note the choice of activation functions (e.g., tf.nn.relu and tf.nn.softmax) for different layers.\n",
    "    \n",
    "7- Custom Loss Function: Study the custom loss function custom_sparse_categorical_crossentropy. This function computes the loss based on the negative log probabilities.\n",
    "\n",
    "    Understand how the negative log probabilities are calculated.\n",
    "    Observe how the mean loss across the batch is computed.\n",
    "8- Custom Accuracy Metric: Examine the custom_accuracy function, which calculates accuracy as the percentage of correct predictions.\n",
    "\n",
    "    Pay attention to how it compares predicted and true labels to determine accuracy.\n",
    "9- Model Compilation: Observe how the model is compiled using the Adam optimizer or other optimizer or you can write costum optimizer function, the custom loss function, and the custom accuracy metric.\n",
    "\n",
    "    Understand the significance of choosing appropriate optimizer and metrics for the task.\n",
    "10- Model Training: Check the training process using the model.fit method. The model is trained for a specified number of epochs and with a given batch size.\n",
    "\n",
    "11- Model Evaluation: Understand how the trained model is evaluated on the test dataset using the model.evaluate method. The test accuracy is printed as the result.\n",
    "\n",
    "Assignment Tasks (Optional):\n",
    "\n",
    "Experiment with different neural network architectures (e.g., changing the number of units, adding more layers, or trying different activation functions) to see how they affect model performance.\n",
    "\n",
    "Implement your custom loss function or metric and evaluate its impact on model training and performance.\n",
    "\n",
    "If you have a larger dataset, consider adapting this code to work with it by adjusting the input shape and the number of output units.\n",
    "\n",
    "Explore ways to visualize the model's predictions or intermediate layer activations to gain insights into its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789c5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.2345 - custom_accuracy: 0.9314\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0984 - custom_accuracy: 0.9696\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0670 - custom_accuracy: 0.9794\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0506 - custom_accuracy: 0.9831\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0392 - custom_accuracy: 0.9874\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0312 - custom_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0258 - custom_accuracy: 0.9916\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0210 - custom_accuracy: 0.9928\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0192 - custom_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.0155 - custom_accuracy: 0.9945\n",
      "313/313 [==============================] - 0s 888us/step - loss: 0.1069 - custom_accuracy: 0.9779\n",
      "Test Loss: 0.10685201734304428, Test Accuracy: 0.9779353141784668\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Loading the MNIST Dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Custome dense layer\n",
    "class CustomDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(CustomDenseLayer, self).__init__()#This line calls the constructor of the parent class \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        # Initialize weights and biases\n",
    "        self.weight = self.add_weight(\"weight\", shape=(input_dim, self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.bias = self.add_weight(\"bias\", shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Perform matrix multiplication and add biases\n",
    "        output = tf.matmul(inputs, self.weight) + self.bias\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Neural Network Architecture\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Input shape for 28x28 for MNIST images\n",
    "    CustomDenseLayer(units=128, activation=tf.nn.relu),\n",
    "    CustomDenseLayer(units=10, activation=tf.nn.softmax)  # 10 output units for 10 digits of handwritten digits (0 to 9)\n",
    "])\n",
    "\n",
    "# Custom Loss Function\n",
    "def custom_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    neg_prob = -tf.math.log(tf.reduce_sum(y_true * y_pred, axis = - 1))\n",
    "    return tf.reduce_mean(neg_prob)\n",
    "\n",
    "# Custom Accuracy Metric\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_true, axis = - 1), tf.argmax(y_pred, axis = - 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Model Compilation\n",
    "model.compile(optimizer=\"adam\", loss=custom_sparse_categorical_crossentropy, metrics=[custom_accuracy])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ceeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
